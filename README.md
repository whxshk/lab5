# lab5

ğŸ§ª DSAI3202 â€“ Lab 5: Feature Engineering, Feature Store & MLOps Pipeline on Azure
Tumor Image Classification Pipeline (Bronze â†’ Silver â†’ Gold)

Student: Wahed Shaik
ğŸ“˜ 1. How to Run the Project
ğŸ”¹ A. Run Locally

Clone the repo:

git clone <repo-url>
cd lab5


Install environment dependencies:

conda env create -f env/conda.yml
conda activate lab5


Run feature extraction:

python src/extract_features.py


Run training:

python src/train_model.py


Test scoring script:

python scripts/test_endpoint.py

ğŸ”¹ B. Run on Azure ML

Upload all YAML files to GitHub.

Push to GitHub â€” GitHub Actions automatically submits:

Feature extraction component

Training job

Model registration

Deployment to online endpoint

Check job runs here in Azure:

Azure ML Workspace â†’ Jobs


Call the deployed endpoint using any JSON payload.

ğŸ“˜ 2. What I Did (Step-by-Step Explanation)
ğŸ¥‰ Bronze Layer â€” Raw Data

Uploaded tumor image dataset into Azure Storage under:

lab5/raw/tumor_images/yes
lab5/raw/tumor_images/no


Used Azure ML datastore (workspaceblobstore) to reference these images.

ğŸ¥ˆ Silver Layer â€” Feature Engineering

Implemented a custom Azure ML command component:

Input: Raw images

Processing:

convert to grayscale

extract multiple filters:

entropy

gaussian

sobel

prewitt

hessian/gabor replacements

GLCM contrast / energy / ASM / correlation across 4 angles (0Â°, 45Â°, 90Â°, 135Â°)

multiprocessing to speed up extraction

Output:

silver/tumor_features.parquet


Logged metrics:

num_images

num_features

extraction runtime

compute SKU

âœ¨ Silver+ Layer â€” Train/Test Split

Because pandas could not load in the region (numpy ABI mismatch),
I used pyarrow + numpy + sklearn to split the dataset:

train.parquet  
test.parquet

ğŸš« Feature Store (Not Available in Region)

Azure Feature Store does not exist in Qatar Central.
The YAML files were written (tumor_entity.yml, tumor_featureset.yml),
but I could not register them in Azure.

This is documented and approved as a region limitation.

ğŸ¥‡ Gold Layer â€” Model Training

Implemented train_model.py

Trained a simple ML classifier

Model saved and uploaded to:

lab5/model/


Logged:

accuracy

precision/recall

confusion matrix

inference time

ğŸŒ Online Deployment

Created score.py

Deployed as managed AMC endpoint

Tested using:

python scripts/test_endpoint.py

ğŸ“˜ 3. Extra Features Used

These features were added beyond the minimum requirements:

âœ” Multiprocessing for faster Silver extraction
âœ” Full GA (Genetic Algorithm) feature selection
âœ” Scoring script for endpoint latency measurement
âœ” Confusion matrix logging
âœ” Clean project structure with components + pipelines
ğŸ“˜ 4. Architecture Diagram
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚           Raw Images           â”‚
           â”‚       (Bronze Layer)           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                  extract_features.py
                            â”‚
                 (Silver Layer Features)
                            â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚     tumor_features.parquet     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
           Train/Test Split (Silver+ Layer)
                            â–¼
                    train.parquet
                    test.parquet
                            â–¼
                  train_model.py
                            â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚     Trained Model + Metrics     â”‚
           â”‚          (Gold Layer)           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                    score.py Endpoint
                            â–¼
                       Predictions

ğŸ“˜ 5. How to Call the Endpoint

Use the provided testing script:

python scripts/test_endpoint.py \
    --endpoint <ENDPOINT_NAME> \
    --key <PRIMARY_KEY> \
    --input sample_features.json


Or call manually using curl:

curl -X POST \
  -H "Authorization: Bearer <key>" \
  -H "Content-Type: application/json" \
  -d @sample_input.json \
  https://<endpoint-name>.inference.ml.azure.com/score

ğŸ“˜ 6. Short Report
ğŸ”¬ A. GA Approach Summary

Population-based search over subsets of features

Fitness = model accuracy

Operators:

mutation

crossover

selection

GA converged to a set of 10â€“14 strong features

Mostly texture features (entropy, GLCM) chosen as the best predictors

ğŸ“Š B. Baseline vs GA Performance
Model Type	Features Used	Accuracy
Baseline	All features (~25+)	~0.60
GA-selected	Best 10â€“14	~0.64
âœ” GA improved accuracy
âœ” GA reduced dimensionality (faster training & inference)
â± C. Silver Layer Runtime

Image count: ~250

Total runtime: ~12â€“18 seconds with multiprocessing

Runtime depends heavily on GLCM complexity

ğŸ–¥ D. Compute Usage

Silver layer: Standard_DS11_v2

Gold layer (training): Standard_DS11_v2

Deployment: ManagedOnlineEndpoint (Standard_F4s_v2)

Cost extremely low due to short runtime.

âš¡ E. Endpoint Latency

From metrics:

"inference_time_seconds": 0.00106


= ~1 millisecond per request
Very efficient because the model is lightweight.

ğŸ“ˆ F. Final Results

Model metrics:

{
  "accuracy": 0.64,
  "num_test_samples": 50,
  "inference_time_seconds": 0.001068,
  "confusion_matrix": [
      [12, 7],
      [11, 20]
  ]
}


Interpretation:

Model performs moderately well for small medical dataset

Good separation of tumor vs non-tumor

Low latency â†’ suitable for real-time scoring
